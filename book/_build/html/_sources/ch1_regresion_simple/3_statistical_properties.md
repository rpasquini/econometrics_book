## Statistical Properties of Estimated Coefficients

Regression analysis, at its core, attempts to uncover the relationship between a dependent variable (Y) and one or more independent variables (X). This process involves estimating coefficients that represent the strength and direction of these relationships. However, it's crucial to recognize that these coefficients are not absolute truths, but rather estimates derived from a sample of data. Understanding the statistical properties of these estimates is crucial for drawing valid conclusions about the relationships between variables.

**1. Unbiasedness:** The first key property is that regression coefficients are **unbiased estimators**. This means that, on average, the estimated coefficients will equal the true population coefficients. Imagine repeating the regression analysis with different samples from the same population. While individual estimates may vary due to the inherent randomness of sampling, their average will converge towards the true value. This property ensures that our estimates are not systematically skewed in a particular direction, providing a level of confidence in their accuracy.

**Visualizing Unbiasedness:** Imagine we are trying to understand the relationship between the number of bedrooms in a rental property and its price. The true relationship, which we as researchers don't know, might be represented by a specific coefficient for "bedrooms". Let's say the true coefficient is 5,000, meaning that each additional bedroom increases the price by 5,000 units (e.g., pesos). If we were to repeatedly sample different sets of rental properties and run our regression, we would get slightly different estimates for the "bedrooms" coefficient each time. Some estimates might be closer to the true value of 5,000, while others might be slightly higher or lower. However, if we average all of these estimates together, we would find that the average is very close to the true value of 5,000. This demonstrates the concept of unbiasedness â€“ the estimates are not consistently overestimating or underestimating the true value, they are "centered" around it.

**2. Variance:** While unbiased, our estimates will always exhibit some degree of **variance**, which represents the uncertainty surrounding the estimated coefficient. This variance measures how much our estimated coefficients might deviate from the true population coefficients. A higher variance indicates a less precise estimate, meaning the true value could be further away from the estimated value. Conversely, a lower variance suggests a more reliable estimate, where the true value is likely closer to the estimated value. The variance of our estimates is influenced by two key factors:

- **Error in the model:** The presence of unexplained variation in the dependent variable (Y) contributes to the variance of our estimates. This error can be attributed to factors not included in the model (e.g., other variables influencing price beyond bedrooms) or inherent randomness in the data. For example, even with similar bedrooms, some properties might be located in more desirable areas, impacting their price beyond the simple bedroom count. This unexplained variation adds noise to our estimations.
- **Variability of the independent variable:** A greater spread in the values of our independent variable (X) leads to lower variance in our coefficient estimates. This is because a wider range of X values provides more information to estimate the relationship with Y. If we were to analyze properties with a narrow range of bedrooms, for example, only 1 or 2 bedrooms, we would have less data to accurately determine the relationship between bedrooms and price. In contrast, analyzing properties with a broader range, from studios to 5-bedroom houses, would provide more robust data to estimate the impact of bedrooms on price.

**Visualizing Variance:** Think about the scatter plot of our rental data. If the data points are tightly clustered around the regression line, our estimated coefficients will have low variance, indicating a more precise estimate. However, if the data points are scattered widely, our estimated coefficients will have higher variance, reflecting more uncertainty about the true relationship.

**3. Hypothesis Testing:** Hypothesis testing provides a formal framework for assessing the statistical significance of our coefficients. A common hypothesis test examines whether a particular coefficient is equal to zero. This test essentially determines if there is a statistically significant relationship between the independent variable and the dependent variable.

- **The T-statistic:** The T-statistic is a key component of this hypothesis test. It is calculated by dividing the estimated coefficient by its standard error (a measure of the variability of the estimate). A larger T-statistic suggests a stronger relationship, meaning the estimated effect is larger relative to its uncertainty. This also increases the likelihood of rejecting the null hypothesis of no effect.
- **P-value:** The P-value represents the probability of observing the estimated coefficient, or a more extreme value, if the null hypothesis were true. A low P-value (typically less than 0.05) indicates strong evidence against the null hypothesis, leading to its rejection. This suggests a statistically significant relationship between the variables, implying that the observed effect is unlikely to be due to chance.

**Understanding Significance:** A statistically significant coefficient does not imply a "large" or "important" effect in practical terms. It simply means that the observed effect is unlikely to be due to random chance. The magnitude of the effect and its practical implications must be considered separately.

**The interplay of these statistical properties is crucial for interpreting regression results:**

- An unbiased coefficient with low variance provides a reliable estimate of the true population relationship.
- Hypothesis testing allows us to determine the statistical significance of the relationship, helping to separate real effects from random fluctuations.
- These properties are crucial for drawing valid conclusions about the relationships between variables and for making informed decisions based on our findings.

By understanding the statistical properties of estimated coefficients, researchers can move beyond simply calculating a regression line to make more informed inferences about the relationships between variables and to draw meaningful conclusions from their data.